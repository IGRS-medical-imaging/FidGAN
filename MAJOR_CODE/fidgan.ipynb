{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries required\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from torchvision import models, transforms\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "from torchvision.models import vgg19\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TRAIN_DIR = \"/home/imaging/Desktop/kk/GEN/data/maps/train\"  # Change to your Train directory\n",
    "VAL_DIR = \"/home/imaging/Desktop/kk/GEN/data/maps/val\" # Change to your validation directory\n",
    "TEST_DIR = \"/home/imaging/Desktop/kk/GEN/niop\"  # Change to your test images directory\n",
    "SAVE_FOLDER = \"/home/imaging/Desktop/kk/GEN/d1\"  # Change to your save folder\n",
    "GRAPH_SAVE_FOLDER = \"/home/imaging/Desktop/kk/GEN/d2\" #Change to your output losses Graph\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 15\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS_IMG = 3\n",
    "L1_LAMBDA = 100\n",
    "LAMBDA_GP = 10\n",
    "NUM_EPOCHS = 250\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_DISC = \"642discp.pth.tar\" #Directory of checkpoints of discriminator\n",
    "CHECKPOINT_GEN = \"642genp.pth.tar\" #Directory of checkpoints of Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "both_transform = A.Compose(\n",
    "    [A.Resize(width=256, height=256)], additional_targets={\"image0\": \"image\"},\n",
    ")\n",
    "\n",
    "transform_only_input = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ColorJitter(p=0.2),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_only_mask = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset classes\n",
    "class MapDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.list_files = os.listdir(self.root_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_file = self.list_files[index]\n",
    "        img_path = os.path.join(self.root_dir, img_file)\n",
    "        image = np.array(Image.open(img_path))\n",
    "        input_image = image[:, :256, :]\n",
    "        target_image = image[:, 256:, :]\n",
    "\n",
    "        augmentations = both_transform(image=input_image, image0=target_image)\n",
    "        input_image = augmentations[\"image\"]\n",
    "        target_image = augmentations[\"image0\"]\n",
    "\n",
    "        input_image = transform_only_input(image=input_image)[\"image\"]\n",
    "        target_image = transform_only_mask(image=target_image)[\"image\"]\n",
    "\n",
    "        return input_image, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.list_files = os.listdir(self.root_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_file = self.list_files[index]\n",
    "        img_path = os.path.join(self.root_dir, img_file)\n",
    "        image = np.array(Image.open(img_path))\n",
    "        input_image = image[:, :256, :]\n",
    "        target_image = image[:, 256:, :]\n",
    "\n",
    "        augmentations = both_transform(image=input_image, image0=target_image)\n",
    "        input_image = augmentations[\"image\"]\n",
    "        target_image = augmentations[\"image0\"]\n",
    "\n",
    "        input_image = transform_only_input(image=input_image)[\"image\"]\n",
    "        target_image = transform_only_mask(image=target_image)[\"image\"]\n",
    "\n",
    "        return input_image, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\"),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2))\n",
    "            in_channels = feature\n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        x = self.initial(x)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a discriminator instance\n",
    "    D = Discriminator()\n",
    "\n",
    "    # Generate random inputs (batch_size, channels, height, width)\n",
    "    x = torch.randn(1, 3, 256, 256)  # Random input image tensor\n",
    "    y = torch.randn(1, 3, 256, 256)  # Random target image tensor\n",
    "\n",
    "    # Pass the inputs through the discriminator\n",
    "    output = D(x, y)\n",
    "\n",
    "    # Print the output shape\n",
    "    print(output.shape)  # Should be (batch_size, 1, output_height, output_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
    "        super().__init__()\n",
    "        if down:\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
    "        else:\n",
    "            self.conv = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        if act == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif act == \"leaky\":\n",
    "            self.activation = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5) if use_dropout else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.activation(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.initial_down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.down1 = Block(features, features * 2, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down2 = Block(features * 3, features * 4, down=True, act=\"leaky\", use_dropout=False)  # Adjusted input channels\n",
    "        self.down3 = Block(features * 4, features * 8, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down4 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down5 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down6 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.bottleneck = nn.Sequential(nn.Conv2d(features * 8, features * 8, 4, 2, 1), nn.ReLU())\n",
    "\n",
    "        self.up1 = Block(features * 8, features * 8, down=False, act=\"relu\", use_dropout=True)\n",
    "        self.up2 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True)\n",
    "        self.up3 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True)\n",
    "        self.up4 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.up5 = Block(features * 8 * 2, features * 4, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.up6 = Block(features * 4 * 2, features * 2, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.final_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features * 2, in_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.downsample = nn.Conv2d(features, features, kernel_size=3, stride=2, padding=1)  # Adjusted downsampling layer\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.initial_down(x)\n",
    "        d2 = self.down1(d1)\n",
    "        d1_downsampled = self.downsample(d1)  # Downsample d1 to match d2\n",
    "        d2_concat = torch.cat([d2, d1_downsampled], 1)\n",
    "        d3 = self.down2(d2_concat)\n",
    "        d4 = self.down3(d3)\n",
    "        d5 = self.down4(d4)\n",
    "        d6 = self.down5(d5)\n",
    "        d7 = self.down6(d6)\n",
    "        bottleneck = self.bottleneck(d7)\n",
    "        up1 = self.up1(bottleneck)\n",
    "        up2 = self.up2(torch.cat([up1, d7], 1))\n",
    "        up3 = self.up3(torch.cat([up2, d6], 1))\n",
    "        up4 = self.up4(torch.cat([up3, d5], 1))\n",
    "        up5 = self.up5(torch.cat([up4, d4], 1))\n",
    "        up6 = self.up6(torch.cat([up5, d3], 1))\n",
    "        up7 = self.up7(torch.cat([up6, d2], 1))\n",
    "        return self.final_up(torch.cat([up7, d1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator(in_channels=3, features=64)\n",
    "preds = model(x)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory to save graphs exists\n",
    "os.makedirs(GRAPH_SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "# Function to calculate style loss\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.vgg = models.vgg19(pretrained=True).features[:36].eval()  # Use first 4 conv layers of VGG19\n",
    "\n",
    "    def forward(self, y_fake, y):\n",
    "        return torch.mean((self.vgg(y_fake) - self.vgg(y)**2))\n",
    "\n",
    "#Train function\n",
    "def train_fn(disc, gen, loader, opt_disc, opt_gen, l1_loss, bce, style_loss, g_scaler, d_scaler):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "\n",
    "    for idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        with torch.cuda.amp.autocast():\n",
    "            y_fake = gen(x)\n",
    "            D_real = disc(x, y)\n",
    "            D_real_loss = bce(D_real, torch.ones_like(D_real))\n",
    "            D_fake = disc(x, y_fake.detach())\n",
    "            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n",
    "            D_loss = (D_real_loss + D_fake_loss) / 2\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # Train Generator\n",
    "        with torch.cuda.amp.autocast():\n",
    "            D_fake = disc(x, y_fake)\n",
    "            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n",
    "            L1 = l1_loss(y_fake, y) * L1_LAMBDA\n",
    "            Style = style_loss(y_fake, y)  # Calculate style loss\n",
    "            G_loss = G_fake_loss + L1 + Style\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        G_losses.append(G_loss.item())\n",
    "        D_losses.append(D_loss.item())\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            loop.set_postfix(D_real=torch.sigmoid(D_real).mean().item(), D_fake=torch.sigmoid(D_fake).mean().item())\n",
    "\n",
    "    return G_losses, D_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_some_examples(gen, test_loader, epoch, folder):\n",
    "    x, y = next(iter(test_loader))\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        y_fake = gen(x)\n",
    "        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
    "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
    "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n",
    "        if epoch == 1:\n",
    "            save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    \n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def main():\n",
    "    disc = Discriminator(in_channels=3).to(DEVICE)\n",
    "    gen = Generator(in_channels=3, features=64).to(DEVICE)\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    l1_loss = nn.L1Loss()\n",
    "    style_loss = StyleLoss().to(DEVICE)\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE)\n",
    "        load_checkpoint(CHECKPOINT_DISC, disc, opt_disc, LEARNING_RATE)\n",
    "\n",
    "    train_dataset = MapDataset(root_dir=TRAIN_DIR)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    g_scaler = torch.cuda.amp.GradScaler()\n",
    "    d_scaler = torch.cuda.amp.GradScaler()\n",
    "    val_dataset = MapDataset(root_dir=VAL_DIR)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    test_dataset = TestDataset(root_dir=TEST_DIR)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "\n",
    "    plt.ion()  # Turn on interactive mode for live plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        g_loss, d_loss = train_fn(disc, gen, train_loader, opt_disc, opt_gen, l1_loss, bce, style_loss, g_scaler, d_scaler)\n",
    "        G_losses.extend(g_loss)\n",
    "        D_losses.extend(d_loss)\n",
    "\n",
    "        if SAVE_MODEL and epoch % 3 == 0:\n",
    "            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
    "            save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC)\n",
    "\n",
    "        save_some_examples(gen, test_loader, epoch, folder=SAVE_FOLDER)\n",
    "\n",
    "        # Update live plot\n",
    "        clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.set_title(\"Generator and Discriminator Loss During Training\")\n",
    "        ax.plot(G_losses, label=\"G\")\n",
    "        ax.plot(D_losses, label=\"D\")\n",
    "        ax.set_xlabel(\"iterations\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend()\n",
    "        plt.draw()\n",
    "        \n",
    "        # Save the current plot as an image\n",
    "        plt.savefig(os.path.join(GRAPH_SAVE_FOLDER, f\"epoch_{epoch}.png\"))\n",
    "\n",
    "    plt.ioff()  # Turn off interactive mode\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the generator\n",
    "generator = Generator()\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('642genp.pth.tar')\n",
    "\n",
    "# Use the correct key to load the model weights\n",
    "generator.load_state_dict(checkpoint['state_dict'])\n",
    "generator.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the transformation to preprocess the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to match input size expected by the model\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load and preprocess the input image\n",
    "input_image = Image.open('/home/imaging/Desktop/kk/cyxck/resized_fig94.png').convert('RGB')\n",
    "input_tensor = transform(input_image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Generate the output image\n",
    "with torch.no_grad():\n",
    "    output_tensor = generator(input_tensor)\n",
    "\n",
    "# Convert the generated image to a numpy array and plot it\n",
    "output_image = output_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "output_image = (output_image + 1) / 2  # Rescale to [0, 1]\n",
    "\n",
    "# Plot the generated image\n",
    "plt.imshow(output_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
